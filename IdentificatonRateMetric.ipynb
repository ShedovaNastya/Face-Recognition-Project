{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1A1adk56ixU2q6FSTL3Pfvs5iJnLu9c_x",
      "authorship_tag": "ABX9TyNCeFNUbNqF5Gl7J6kUuCNM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShedovaNastya/Face-Recognition-Project/blob/main/IdentificatonRateMetric.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Создадим два набора изображений лиц: query и distractors. Никакие лица из этих наборов не должны содержаться в обучающем и валидационном датасете.\n",
        "Посчитаем косинусные расстояния между лицами, соответствующими одним и тем же людям из query части. Например, пусть одному человеку соответствуют три фото в query: 01.jpg, 02.jpg, 03.jpg. Тогда считаем три косинусных расстояния между всеми тремя парами из этих фото.\n",
        "посчитаем косинусные расстояния между лицами, соответствующими разным людям из query части.\n",
        "посчитаем косинусные расстояния между всеми парами лиц из query и distractors. Т.е. пара — это (лицо из query, лицо из distractors). Всего получится |query|*|distractors| пар.\n",
        "Сложим количества пар, полученных на 2 и 3 шагах. Это количество false пар.\n",
        "Зафиксируем FPR (false positive rate). Пусть, например, будет 0.01. FPR, умноженный на количество false пар из шага 4 — это разрешенное количество false positives, которые мы разрешаем нашей модели. Обозначим это количество через N.\n",
        "Отсортируем все значения косинусных расстояний false пар. N — ое по счету значение расстояния зафиксируем как пороговое расстояние.\n",
        "Посчитаем количество positive пар с шага 1, которые имеют косинусное расстояние меньше, чем пороговое расстояние. Поделим это количество на общее количество positive пар с шага 1. Это будет TPR (true positive rate) — итоговое значение нашей метрики.\n",
        "Такая метрика обычно обозначается как TPR@FPR=0.01. FPR может быть разным. Приразных FPR будет получаться разное TPR.\n",
        "\n",
        "Смысл этой метрики в том, что мы фиксируем вероятность ошибки вида false positive, т.е. когда \"сеть сказала, что это один и тот же человек, но это не так\", считаем порог косинусного расстояния для этого значения ошибки, потом берем все positive пары и смотрим, у скольких из них расстояние меньше этого порога. Т.е. насколько точно наша сеть ищет похожие лица при заданной вероятности ошибки вида false positive."
      ],
      "metadata": {
        "id": "LJRT0X3Zdj9Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#IMPORT MODULE"
      ],
      "metadata": {
        "id": "RmMRctiWFF0Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchmetrics\n",
        "\n",
        "import torch\n",
        "import os\n",
        "from IPython.display import clear_output\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import torchvision.transforms as T\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.models import list_models, get_model\n",
        "from torchmetrics.functional import pairwise_cosine_similarity\n",
        "from tqdm.notebook import tqdm as bar\n",
        "from copy import deepcopy\n",
        "from collections import defaultdict"
      ],
      "metadata": {
        "id": "ekwVHaNyA7uQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "979dc5db-20fb-444d-e09f-6e77fb9bc9be"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchmetrics\n",
            "  Downloading torchmetrics-1.3.2-py3-none-any.whl (841 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m841.5/841.5 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (1.25.2)\n",
            "Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (24.0)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (2.2.1+cu121)\n",
            "Collecting lightning-utilities>=0.8.0 (from torchmetrics)\n",
            "  Downloading lightning_utilities-0.11.2-py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (67.7.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.10.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (3.13.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.10.0->torchmetrics)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.10.0->torchmetrics)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.10.0->torchmetrics)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.10.0->torchmetrics)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.10.0->torchmetrics)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.10.0->torchmetrics)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.10.0->torchmetrics)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.10.0->torchmetrics)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.10.0->torchmetrics)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch>=1.10.0->torchmetrics)\n",
            "  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.10.0->torchmetrics)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->torchmetrics)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->torchmetrics) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->torchmetrics) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, lightning-utilities, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torchmetrics\n",
            "Successfully installed lightning-utilities-0.11.2 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105 torchmetrics-1.3.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mQj-02S26RrA",
        "outputId": "a0babe18-14ca-4c05-b2ab-b645e5bb7e23"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !unzip /content/drive/MyDrive/celebA_train_500.zip\n",
        "!unzip /content/drive/MyDrive/celebA_ir.zip\n",
        "clear_output()"
      ],
      "metadata": {
        "id": "oQR-Ne0-rp-t"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Identificaton rate metric"
      ],
      "metadata": {
        "id": "9PIEjMa_k8-a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "f = open('./celebA_ir/celebA_anno_query.csv', 'r')\n",
        "query_lines = f.readlines()[1:]\n",
        "f.close()\n",
        "query_lines = [x.strip().split(',') for x in query_lines]\n",
        "# plain list of image names from query. Neede to compute embeddings for query\n",
        "query_img_names = ['./celebA_ir/celebA_query/' + x[0] for x in query_lines]\n",
        "\n",
        "query_dict = defaultdict(list)\n",
        "for img_name, img_class in query_lines:# словарь ключ- id, значения- изображения, принадлежащие id\n",
        "  query_dict[img_class].append('./celebA_ir/celebA_query/' + img_name)\n",
        "\n",
        "distractors_img_names =['./celebA_ir/celebA_distractors/' + i for i in os.listdir('./celebA_ir/celebA_distractors')]#хранятся названия изображений из distractors\n"
      ],
      "metadata": {
        "id": "_jshj-69k8PY"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query_dict"
      ],
      "metadata": {
        "id": "tmv5BRdtG7fn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class id_rate_dataset(torch.utils.data.Dataset):\n",
        "  def __init__(self, images_list, transform):\n",
        "    self.images_list = images_list\n",
        "    self.transform = transform\n",
        "  def __len__(self):\n",
        "    return len(self.images_list)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    image_name = self.images_list[idx]\n",
        "    image = Image.open(image_name)\n",
        "    image = self.transform(image)\n",
        "    return image\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "lOBciDdDJUTZ"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Модель для оценки\n",
        "m1 = get_model(\"efficientnet_b0\", weights=\"IMAGENET1K_V1\")\n",
        "m1.classifier[1] = torch.nn.Linear(in_features=1280, out_features=500, bias=True)\n",
        "device = 'cpu'\n",
        "m1 = m1.to(device)\n",
        "\n",
        "\n",
        "filename = '/content/drive/MyDrive/eff_net_param_m1.pth'\n",
        "m1.load_state_dict(torch.load(filename, map_location=device))\n",
        "\n",
        "m1.classifier = m1.classifier[:1]\n"
      ],
      "metadata": {
        "id": "CB63I02-ZIhj"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!du -h celebA_ir/celebA_query/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g1gIEwc7ca86",
        "outputId": "1d42e9f6-8766-488f-e132-55d34ca15ccd"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4.9M\tcelebA_ir/celebA_query/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_query_dict = {\n",
        "    2876: ['1.jpg', '2.jpg', '3.jpg'],\n",
        "    5674: ['5.jpg'],\n",
        "    864:  ['9.jpg', '10.jpg'],\n",
        "}\n",
        "test_query_img_names = ['1.jpg', '2.jpg', '3.jpg', '5.jpg', '9.jpg', '10.jpg']\n",
        "test_query_embeddings = torch.tensor([\n",
        "                    [1.56, 6.45,  -7.68],\n",
        "                    [-1.1 , 6.11,  -3.0],\n",
        "                    [-0.06,-0.98,-1.29],\n",
        "                    [8.56, 1.45,  1.11],\n",
        "                    [0.7,  1.1,   -7.56],\n",
        "                    [0.05, 0.9,   -2.56],\n",
        "])\n",
        "\n",
        "test_distractors_img_names = ['11.jpg', '12.jpg', '13.jpg', '14.jpg', '15.jpg']\n",
        "\n",
        "test_distractors_embeddings = torch.tensor([\n",
        "                    [0.12, -3.23, -5.55],\n",
        "                    [-1,   -0.01, 1.22],\n",
        "                    [0.06, -0.23, 1.34],\n",
        "                    [-6.6, 1.45,  -1.45],\n",
        "                    [0.89,  1.98, 1.45],\n",
        "])\n"
      ],
      "metadata": {
        "id": "2qXofQ3fQwU5"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_embeddings(model, images_list):#функция считает выход ембеддингового слоя\n",
        "  '''\n",
        "  compute embeddings from the trained model for list of images.\n",
        "  params:\n",
        "    model: trained nn model that takes images and outputs embeddings\n",
        "    images_list: list of images paths to compute embeddings for\n",
        "  output:\n",
        "    list: list of model embeddings. Each embedding corresponds to images\n",
        "          names from images_list\n",
        "  '''\n",
        "  out_data = []\n",
        "  dataset = id_rate_dataset(images_list,  T.Compose([T.Resize((224, 224)), T.ToTensor()]))\n",
        "  loader = DataLoader(dataset, batch_size = 30, shuffle = False)\n",
        "\n",
        "  model.eval()\n",
        "  correct = 0\n",
        "  with torch.no_grad():\n",
        "    for data in loader:\n",
        "      data = data.to(device)\n",
        "\n",
        "      out_data.append(model(data))\n",
        "  return torch.cat(out_data)\n",
        "\n",
        "\n",
        "#compute_embeddings(m1, query_img_names)"
      ],
      "metadata": {
        "id": "clHeeoVGrBjM"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9cwFbPeYrv7g"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mMZsxj7ToMJ2",
        "outputId": "ce742926-d620-477c-bdd2-b76b0c09e7c0"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "317653\n",
            "[ 0.9419636   0.81423724  0.80625534 ... -0.11651252 -0.11936088\n",
            " -0.12310144]\n",
            "317653\n",
            "[ 0.9419636   0.81423724  0.80625534 ... -0.11651252 -0.11936088\n",
            " -0.12310144]\n",
            "317653\n",
            "[ 0.9419636   0.81423724  0.80625534 ... -0.11651252 -0.11936088\n",
            " -0.12310144]\n",
            "317653\n",
            "[ 0.9419636   0.81423724  0.80625534 ... -0.11651252 -0.11936088\n",
            " -0.12310144]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def cosine_similarity(list_em, list_em2):\n",
        "  em1 = torch.Tensor(list_em)\n",
        "  em2 = torch.Tensor(list_em2)\n",
        "  similarity= pairwise_cosine_similarity(em1,em2)\n",
        "  return similarity"
      ],
      "metadata": {
        "id": "iCA3sBKafX4p"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.randn(size=(3,3))\n",
        "cosine_similarity(a,a)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KkBmFleYl_4b",
        "outputId": "74e438ff-a20d-4fc3-dff9-c2416c103490"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 1.0000, -0.2525, -0.7671],\n",
              "        [-0.2525,  1.0000, -0.3054],\n",
              "        [-0.7671, -0.3054,  1.0000]])"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_cosine_query_pos(query_dict, query_img_names, query_embeddings):\n",
        "  '''\n",
        "  compute cosine similarities between positive pairs from query (stage 1)\n",
        "  params:\n",
        "    query_dict: dict {class: [image_name_1, image_name_2, ...]}. Key: class in\n",
        "                the dataset. Value: images corresponding to that class\n",
        "    query_img_names: list of images names\n",
        "    query_embeddings: list of embeddings corresponding to query_img_names\n",
        "  output:\n",
        "    list of floats: similarities between embeddings czorresponding\n",
        "                    to the same people from query list\n",
        "\n",
        "    '''\n",
        "  similarity = []\n",
        "  images = {name:idx for idx, name in enumerate(query_img_names)}#словарь ключ - имя изображения, значение - индекс в query_img_names\n",
        "  for id in query_dict.keys():#идем по каждому лицу\n",
        "    emb = []\n",
        "    img_id = query_dict[id]#получаю список изображений, принадлежащих одному лицу\n",
        "    for img in img_id:# иду по кааждому изображению\n",
        "      emb.append(query_embeddings[images[img]].unsqueeze(0))#добавляю в список ембеддинг каждой фотки, unsqeeze - добавляет размерность фиктивную [[...]] [[...]]\n",
        "    emb = torch.cat(emb)#[[...],[...]]\n",
        "    matrix = cosine_similarity(emb, emb)\n",
        "    mask = torch.triu(torch.ones_like(matrix), diagonal=1)# triu- верх.треугольник, ones_like - все единицы, diagonal=1 - по нулям\n",
        "    matrix = matrix[mask == 1]\n",
        "    if len(matrix) > 0:# обрабатываем случай когда у одного лица лолько одно изображение,добавлять тогда не будем\n",
        "      similarity.append(matrix)\n",
        "  return torch.cat(similarity)\n",
        "\n",
        "\n",
        "# cosine_query_pos = compute_cosine_query_pos(query_dict, query_img_names,\n",
        "#                                             query_embeddings)\n",
        "test_cosine_query_pos = compute_cosine_query_pos(test_query_dict, test_query_img_names,\n",
        "                                            test_query_embeddings)\n",
        "true_cosine_query_pos = torch.Tensor([0.8678237233650096, 0.21226104378511604,\n",
        "                         -0.18355866977496182, 0.9787437979250561])\n",
        "assert np.allclose(sorted(test_cosine_query_pos), sorted(true_cosine_query_pos)), \\\n",
        "      \"A mistake in compute_cosine_query_pos function\"\n",
        "print(test_cosine_query_pos)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "piVEvBnMUXcu",
        "outputId": "9c8e8d9d-fd17-4158-d63d-907862825bfa"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([ 0.8678,  0.2123, -0.1836,  0.9787])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_cosine_query_neg(query_dict, query_img_names, query_embeddings):\n",
        "  '''\n",
        "  compute cosine similarities between negative pairs from query (stage 2)\n",
        "  params:\n",
        "    query_dict: dict {class: [image_name_1, image_name_2, ...]}. Key: class in\n",
        "                the dataset. Value: images corresponding to that class\n",
        "    query_img_names: list of images names\n",
        "    query_embeddings: list of embeddings corresponding to query_img_names\n",
        "  output:\n",
        "    list of floats: similarities between embeddings corresponding\n",
        "                    to different people from query list\n",
        "  '''\n",
        "  similarity = []\n",
        "  keys_id = list(query_dict.keys())\n",
        "  images = {name:idx for idx, name in enumerate(query_img_names)}#словарь ключ - имя изображения, значение - индекс в query_img_names\n",
        "  for id1 in range(len(keys_id)):\n",
        "    emb1 = []\n",
        "    images1 = query_dict[keys_id[id1]]\n",
        "    for img in images1:\n",
        "      emb1.append(query_embeddings[images[img]].unsqueeze(0))\n",
        "    emb1 = torch.cat(emb1)\n",
        "    for id2 in range(id1+1,len(keys_id)):\n",
        "      images2 = query_dict[keys_id[id2]]\n",
        "      emb2 = []\n",
        "      for img in images2:\n",
        "        emb2.append(query_embeddings[images[img]].unsqueeze(0))\n",
        "      emb2 = torch.cat(emb2)\n",
        "      # print(cosine_similarity(emb1, emb2))\n",
        "      similarity.append(cosine_similarity(emb1, emb2).view(-1))#здесь разворачиваем двумерный тензор в массивчик тензоров через view\n",
        "      # break\n",
        "    # break\n",
        "\n",
        "  return torch.cat(similarity)\n",
        "\n",
        "\n",
        "\n",
        "test_cosine_query_neg = compute_cosine_query_neg(test_query_dict, test_query_img_names,\n",
        "                                            test_query_embeddings)\n",
        "\n",
        "\n",
        "\n",
        "true_cosine_query_neg = [0.15963231223161822, 0.8507997093616965, 0.9272761484302097,\n",
        "                         -0.0643994061127092, 0.5412660901220571, 0.701307100338029,\n",
        "                         -0.2372575528216902, 0.6941032794522218, 0.549425446066643,\n",
        "                         -0.011982733001947084, -0.0466679194884999]\n",
        "print(test_cosine_query_neg)\n",
        "assert np.allclose(sorted(test_cosine_query_neg), sorted(true_cosine_query_neg)), \\\n",
        "      \"A mistake in compute_cosine_query_neg function\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4EfPb_8bSbyB",
        "outputId": "3e604102-9449-4834-8947-20460bfca0d8"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([ 0.1596, -0.0644, -0.2373,  0.8508,  0.9273,  0.5413,  0.7013,  0.6941,\n",
            "         0.5494, -0.0120, -0.0467])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_cosine_query_distractors(query_embeddings, distractors_embeddings):\n",
        "  '''\n",
        "  compute cosine similarities between negative pairs from query and distractors\n",
        "  (stage 3)\n",
        "  params:\n",
        "    query_embeddings: list of embeddings corresponding to query_img_names\n",
        "    distractors_embeddings: list of embeddings corresponding to distractors_img_names\n",
        "  output:\n",
        "    list of floats: similarities between pairs of people (q, d), where q is\n",
        "                    embedding corresponding to photo from query, d —\n",
        "                    embedding corresponding to photo from distractors\n",
        "  '''\n",
        "  similarity = []\n",
        "  batch_size = 50\n",
        "  for q in range(0, len(query_embeddings), batch_size):\n",
        "    emb1 = query_embeddings[q: q + batch_size]\n",
        "    for d in range(0,len(distractors_embeddings), batch_size):\n",
        "      emb2 = distractors_embeddings[d:d+batch_size]\n",
        "      similarity.append(cosine_similarity(emb1, emb2).view(-1))\n",
        "  return torch.cat(similarity)\n",
        "\n",
        "\n",
        "\n",
        "  raise NotImplementedError\n",
        "\n",
        "\n",
        "test_cosine_query_distractors = compute_cosine_query_distractors(test_query_embeddings,\n",
        "                                                            test_distractors_embeddings)\n",
        "\n",
        "\n",
        "true_cosine_query_distractors = [0.3371426578637511, -0.6866465610863652, -0.8456563512871669,\n",
        "                                 0.14530087113136106, 0.11410510307646118, -0.07265097629002357,\n",
        "                                 -0.24097699660707042,-0.5851992679925766, 0.4295494455718534,\n",
        "                                 0.37604478596058194, 0.9909483738948858, -0.5881093317868022,\n",
        "                                 -0.6829712976642919, 0.07546364489032083, -0.9130970963915521,\n",
        "                                 -0.17463101988684684, -0.5229363015558941, 0.1399896725311533,\n",
        "                                 -0.9258034013399499, 0.5295114163723346, 0.7811585442749943,\n",
        "                                 -0.8208760031249596, -0.9905139680301821, 0.14969764653247228,\n",
        "                                 -0.40749654525418444, 0.648660814944824, -0.7432584300096284,\n",
        "                                 -0.9839696492435877, 0.2498741082804709, -0.2661183373780491]\n",
        "assert np.allclose(sorted(test_cosine_query_distractors), sorted(true_cosine_query_distractors)), \\\n",
        "      \"A mistake in compute_cosine_query_distractors function\""
      ],
      "metadata": {
        "id": "DFy0QEOzEI9D"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_cosine_query_pos"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jsZUxCpXEcd4",
        "outputId": "7984c382-c0dd-4e61-a398-c7cd423d0ff7"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 0.8678,  0.2123, -0.1836,  0.9787])"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_ir(cosine_query_pos, cosine_query_neg, cosine_query_distractors,\n",
        "               fpr=0.1):\n",
        "  '''\n",
        "  compute identification rate using precomputer cosine similarities between pairs\n",
        "  at given fpr\n",
        "  params:\n",
        "    cosine_query_pos: cosine similarities between positive pairs from query\n",
        "    cosine_query_neg: cosine similarities between negative pairs from query\n",
        "    cosine_query_distractors: cosine similarities between negative pairs\n",
        "                              from query and distractors\n",
        "    fpr: false positive rate at which to compute TPR\n",
        "  output:\n",
        "    float: threshold for given fpr\n",
        "    float: TPR at given FPR\n",
        "  '''\n",
        "  cnt_false = len(cosine_query_neg) + len(cosine_query_distractors)\n",
        "  n = int(cnt_false * fpr)\n",
        "  false_pars = np.concatenate([cosine_query_neg.cpu().numpy(), cosine_query_distractors.cpu().numpy()])\n",
        "  false_pars = np.sort(false_pars)[::-1]\n",
        "  threshold = false_pars[n]\n",
        "  # print(n)\n",
        "  # print(false_pars)\n",
        "  cosine_query_pos = cosine_query_pos.cpu().numpy()\n",
        "  num_true_positive = np.sum(cosine_query_pos >= threshold)\n",
        "  return threshold, num_true_positive/ len(cosine_query_pos)"
      ],
      "metadata": {
        "id": "cafA9JUmHeNo"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_thr = []\n",
        "test_tpr = []\n",
        "for fpr in [0.5, 0.3, 0.1]:\n",
        "  x, y = compute_ir(test_cosine_query_pos, test_cosine_query_neg,\n",
        "                    test_cosine_query_distractors, fpr=fpr)\n",
        "  test_thr.append(x)\n",
        "  test_tpr.append(y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YGYumzMjeGET",
        "outputId": "a70ea292-063e-4693-ee2b-c057a20863fc"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20\n",
            "[ 0.9909484   0.9272762   0.8507998   0.78115857  0.70130706  0.69410324\n",
            "  0.6486608   0.5494254   0.5412661   0.52951145  0.4295494   0.37604478\n",
            "  0.33714268  0.24987411  0.15963233  0.14969766  0.1453009   0.13998966\n",
            "  0.11410506  0.07546364 -0.01198272 -0.04666792 -0.06439941 -0.072651\n",
            " -0.17463103 -0.23725754 -0.24097699 -0.2661184  -0.4074966  -0.5229363\n",
            " -0.5851992  -0.5881093  -0.68297124 -0.68664664 -0.7432585  -0.82087606\n",
            " -0.84565634 -0.91309714 -0.9258034  -0.98396957 -0.9905139 ]\n",
            "12\n",
            "[ 0.9909484   0.9272762   0.8507998   0.78115857  0.70130706  0.69410324\n",
            "  0.6486608   0.5494254   0.5412661   0.52951145  0.4295494   0.37604478\n",
            "  0.33714268  0.24987411  0.15963233  0.14969766  0.1453009   0.13998966\n",
            "  0.11410506  0.07546364 -0.01198272 -0.04666792 -0.06439941 -0.072651\n",
            " -0.17463103 -0.23725754 -0.24097699 -0.2661184  -0.4074966  -0.5229363\n",
            " -0.5851992  -0.5881093  -0.68297124 -0.68664664 -0.7432585  -0.82087606\n",
            " -0.84565634 -0.91309714 -0.9258034  -0.98396957 -0.9905139 ]\n",
            "4\n",
            "[ 0.9909484   0.9272762   0.8507998   0.78115857  0.70130706  0.69410324\n",
            "  0.6486608   0.5494254   0.5412661   0.52951145  0.4295494   0.37604478\n",
            "  0.33714268  0.24987411  0.15963233  0.14969766  0.1453009   0.13998966\n",
            "  0.11410506  0.07546364 -0.01198272 -0.04666792 -0.06439941 -0.072651\n",
            " -0.17463103 -0.23725754 -0.24097699 -0.2661184  -0.4074966  -0.5229363\n",
            " -0.5851992  -0.5881093  -0.68297124 -0.68664664 -0.7432585  -0.82087606\n",
            " -0.84565634 -0.91309714 -0.9258034  -0.98396957 -0.9905139 ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "true_thr = [-0.011982733001947084, 0.3371426578637511, 0.701307100338029]\n",
        "assert np.allclose(np.array(test_thr), np.array(true_thr)), \"A mistake in computing threshold\"\n",
        "\n",
        "true_tpr = [0.75, 0.5, 0.5]\n",
        "assert np.allclose(np.array(test_tpr), np.array(true_tpr)), \"A mistake in computing tpr\""
      ],
      "metadata": {
        "id": "k64ONGtDeMMH"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_thr"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tu34nQTeGZya",
        "outputId": "7e67b215-86f2-4f55-8974-a4e05524cbef"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[-0.011982724, 0.33714268, 0.70130706]"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# query_embeddings = compute_embeddings(m1, query_img_names)\n",
        "# distractors_embeddings = compute_embeddings(m1, distractors_img_names)\n",
        "\n",
        "cosine_query_pos = compute_cosine_query_pos(query_dict, query_img_names, query_embeddings)\n",
        "cosine_query_neg = compute_cosine_query_neg(query_dict, query_img_names, query_embeddings)\n",
        "cosine_query_distractors = compute_cosine_query_distractors(query_embeddings, distractors_embeddings)\n",
        "\n",
        "thr = []\n",
        "tpr = []\n",
        "\n",
        "for fpr in [0.5, 0.1, 0.05, 0.01]:#0.01 при 1% когда он допускает разных людей одинаковыми, то при 0,44 верно предсказывает на изображении одного и того же человека\n",
        "  x, y = compute_ir(cosine_query_pos, cosine_query_neg, cosine_query_distractors, fpr)\n",
        "  thr.append(x)\n",
        "  tpr.append(y)"
      ],
      "metadata": {
        "id": "6urC6ML4o6NL"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tpr"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QFHIE04To9Vg",
        "outputId": "d9598d15-b933-4ee9-8394-c1f1d6cecc71"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.9703824468446437,\n",
              " 0.7847972284491542,\n",
              " 0.6785544460294817,\n",
              " 0.4408667889409687]"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    }
  ]
}