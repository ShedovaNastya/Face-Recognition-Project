{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO8CnIo5mD0MFrU4Du7ZN/w",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShedovaNastya/Face-Recognition-Project/blob/main/TripletLoss.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Triplet Loss — это один из лоссов для contrastive learning. Чтобы учить модель с помощью этого лосса, модели не нужен последний классификационный слой. Этот лосс работает прямо с эмбеддингами $x_i$ элементов, которые выдает модель."
      ],
      "metadata": {
        "id": "kPy1LODHR6JC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Снова скажем, что идея лосса — заставить эмбеддинги лиц одного человека быть более близкими по некоторому расстоянию, а эмбеддинги лиц разных людей — далекими друг от друга. Общая формула лосса выглядит так:\n",
        "\n",
        "$$L(e, p, n) = max\\{d(a, p) - d(a, n) + margin, 0\\},$$\n",
        "\n",
        "здесь\n",
        "- $e$ — эмбеддинг входного лица (output модели)\n",
        "- $p$ — \"positive\" эмбеддинг для входного лица (т.е. эмбеддинг такого элемента, что мы хотим, чтобы $e$ и $p$ были близки. В нашем случае это значит, что $e$ и $p$ должны быть выходами сети на два разных фото одного и того же человека).\n",
        "- $n$ — \"negative\" эмбеддинг для входного лица (т.е. эмбеддинг такого элемента, что мы хотим, чтобы $e$ и $p$ были далеки. В нашем случае это значит, что $e$ и $p$ должны быть выходами сети на два разных фото разных людей).\n",
        "- $d(x, y)$ — метрика расстояния между эмбеддингами, по которой мы их сравниваем.\n",
        "- margin — гиперпараметр, который заставляет $d(a, p)$ и $d(a, n)$ быть еще дальше друг от друга.\n",
        "\n",
        "**Эмбеддинги $e$, $p$ и $n$ нужно нормализовать, прежде чем подавать в лосс-функцию**.\n",
        "\n",
        "У TripletLoss есть куча разных вариаций. В некоторых из них больше гиперпараметров, в других предлагают использовать больше одного позитивного и негативного эмбеддинга за раз. Где-то предлагается умный способ выбора negative эмбеддинга (например, выбирается такой, на котором нейросеть пока плохо работает, т.е. считает $e$ и $n$ близкими).\n",
        "\n",
        "Пример реализации TripletLoss можно найти [здесь](https://pytorch.org/docs/stable/generated/torch.nn.TripletMarginWithDistanceLoss.html#torch.nn.TripletMarginWithDistanceLoss).\n",
        "\n",
        "Будьте готовы, что TripletLoss придется настраивать, чтобы добиться хорошего результата при обучении сети.\n"
      ],
      "metadata": {
        "id": "6cMS9QhvTE2q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Что нужно учесть при реализации Triplet Loss**:\n",
        "- при обучении мы обычно хотим следить за ходом обучения модели, считая какую-то метрику качества. Тут у нас больше нет классификационного слоя, так что accuracy мы считать не можем. Нужно придумать, как в случае Triplet Loss считать метрику качества на вализации в течение обучения. Подумайте, как можно это сделать?\n",
        "- скорее всего, чтобы обучить сеть на Triplet Loss, придется написать кастомный Dalaset/Dataloader, который будет возвращать тройки элементов (anchor, positive, negative).\n",
        "- не забудьте нормализовать эмбеддинги перед подсчетом лосса! Это можно сделать руками, а можно, например, добавить в конец сети batchnorm без обучаемых параметров."
      ],
      "metadata": {
        "id": "XYTA2a28Vwvk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Доплитература по Triplet Loss**:\n",
        "\n",
        "- Идея TripletLoss: https://en.wikipedia.org/wiki/Triplet_loss\n",
        "- Хорошая статья про batch mining techniques для выбора positive и negative элементов: https://omoindrot.github.io/triplet-loss#triplet-mining\n",
        "- Реализация TripletLoss в PyTorch : https://pytorch.org/docs/stable/generated/torch.nn.TripletMarginWithDistanceLoss.html#torch.nn.TripletMarginWithDistanceLoss\n",
        "- Еще одна реализация TripletLoss: https://github.com/alfonmedela/triplet-loss-pytorch/blob/master/loss_functions/triplet_loss.py"
      ],
      "metadata": {
        "id": "sDtNoSCpVbKF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "zyd6kT1O8j7j"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "VH1LIYmKJAs-"
      },
      "outputs": [],
      "source": [
        "def _pairwise_distances(embeddings, squared=False):\n",
        "    \"\"\"Compute the 2D matrix of distances between all the embeddings.\n",
        "\n",
        "    Args:\n",
        "        embeddings: tensor of shape (batch_size, embed_dim)\n",
        "        squared: Boolean. If true, output is the pairwise squared euclidean distance matrix.\n",
        "                 If false, output is the pairwise euclidean distance matrix.\n",
        "\n",
        "    Returns:\n",
        "        pairwise_distances: tensor of shape (batch_size, batch_size)\n",
        "    \"\"\"\n",
        "    # Get the dot product between all embeddings\n",
        "    # shape (batch_size, batch_size)\n",
        "    dot_product = torch.matmul(embeddings, torch.transpose(embeddings, 0, 1))\n",
        "\n",
        "    # Get squared L2 norm for each embedding. We can just take the diagonal of `dot_product`.\n",
        "    # This also provides more numerical stability (the diagonal of the result will be exactly 0).\n",
        "    # shape (batch_size,)\n",
        "    square_norm = torch.diagonal(dot_product, 0)\n",
        "\n",
        "    # Compute the pairwise distance matrix as we have:\n",
        "    # ||a - b||^2 = ||a||^2  - 2 <a, b> + ||b||^2\n",
        "    # shape (batch_size, batch_size)\n",
        "    distances = torch.unsqueeze(square_norm, 0) - 2.0 * dot_product + torch.unsqueeze(square_norm, 1)\n",
        "\n",
        "    # Because of computation errors, some distances might be negative so we put everything >= 0.0\n",
        "    distances = torch.maximum(distances, torch.tensor(0.0))\n",
        "\n",
        "    if not squared:\n",
        "        # Because the gradient of sqrt is infinite when distances == 0.0 (ex: on the diagonal)\n",
        "        # we need to add a small epsilon where distances == 0.0\n",
        "        mask = torch.tensor(distances == 0.0, dtype=torch.float)\n",
        "        distances = distances + mask * 1e-16\n",
        "\n",
        "        distances = torch.sqrt(distances)\n",
        "\n",
        "        # Correct the epsilon added: set the distances on the mask to be exactly 0.0\n",
        "        distances = distances * (1.0 - mask)\n",
        "\n",
        "    return distances"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def _get_triplet_mask(labels):\n",
        "\n",
        "    mask = torch.zeros(len(labels), len(labels), len(labels), dtype=torch.bool)\n",
        "\n",
        "    for i in range(len(labels)):\n",
        "        for j in range(len(labels)):\n",
        "            for k in range(len(labels)):\n",
        "                if labels[i] == labels[j] and labels[i] != labels[k]:\n",
        "                    mask[i][j][k] = True\n",
        "\n",
        "    return mask"
      ],
      "metadata": {
        "id": "Qz6VfIr2EGGB"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def batch_all_triplet_loss(labels, embeddings, margin, squared=False):\n",
        "    \"\"\"Build the triplet loss over a batch of embeddings.\n",
        "\n",
        "    We generate all the valid triplets and average the loss over the positive ones.\n",
        "\n",
        "    Args:\n",
        "        labels: labels of the batch, of size (batch_size,)\n",
        "        embeddings: tensor of shape (batch_size, embed_dim)\n",
        "        margin: margin for triplet loss\n",
        "        squared: Boolean. If true, output is the pairwise squared euclidean distance matrix.\n",
        "                 If false, output is the pairwise euclidean distance matrix.\n",
        "\n",
        "    Returns:\n",
        "        triplet_loss: scalar tensor containing the triplet loss\n",
        "    \"\"\"\n",
        "    # Get the pairwise distance matrix\n",
        "    pairwise_dist = _pairwise_distances(embeddings, squared=squared)\n",
        "\n",
        "    anchor_positive_dist = torch.unsqueeze(pairwise_dist, 2)\n",
        "    anchor_negative_dist = torch.unsqueeze(pairwise_dist, 1)\n",
        "\n",
        "    # Compute a 3D tensor of size (batch_size, batch_size, batch_size)\n",
        "    # triplet_loss[i, j, k] will contain the triplet loss of anchor=i, positive=j, negative=k\n",
        "    # Uses broadcasting where the 1st argument has shape (batch_size, batch_size, 1)\n",
        "    # and the 2nd (batch_size, 1, batch_size)\n",
        "    triplet_loss = anchor_positive_dist - anchor_negative_dist + margin\n",
        "\n",
        "    # Put to zero the invalid triplets\n",
        "    # (where label(a) != label(p) or label(n) == label(a) or a == p)\n",
        "    mask = _get_triplet_mask(labels)\n",
        "    mask = mask.float()\n",
        "    triplet_loss = torch.mul(mask, triplet_loss)\n",
        "\n",
        "    # Remove negative losses (i.e. the easy triplets)\n",
        "    triplet_loss = torch.maximum(triplet_loss, torch.tensor(0.0))\n",
        "\n",
        "    # Count number of positive triplets (where triplet_loss > 0)\n",
        "    valid_triplets = (triplet_loss > 1e-16).float()\n",
        "    num_positive_triplets = torch.sum(valid_triplets)\n",
        "    num_valid_triplets = torch.sum(mask)\n",
        "    fraction_positive_triplets = num_positive_triplets / (num_valid_triplets + 1e-16)\n",
        "\n",
        "    # Get final mean triplet loss over the positive valid triplets\n",
        "    triplet_loss = torch.sum(triplet_loss) / (num_positive_triplets + 1e-16)\n",
        "\n",
        "    return triplet_loss, fraction_positive_triplets"
      ],
      "metadata": {
        "id": "LHSLm7AiJWfX"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _get_anchor_positive_triplet_mask(labels):\n",
        "    num_samples = len(labels)\n",
        "    mask = [[False for i in range(num_samples)] for j in range(num_samples)]\n",
        "\n",
        "    for i in range(num_samples):\n",
        "        for j in range(num_samples):\n",
        "            if labels[i] == labels[j]:\n",
        "                mask[i][j] = True\n",
        "\n",
        "    return mask"
      ],
      "metadata": {
        "id": "NAyG5EqFMK0Z"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _get_anchor_negative_triplet_mask(labels):\n",
        "    num_samples = labels.shape[0]\n",
        "    mask = [[False for i in range(num_samples)] for j in range(num_samples)]\n",
        "\n",
        "    for i in range(num_samples):\n",
        "        for j in range(num_samples):\n",
        "            if labels[i] != labels[j] and i != j:\n",
        "                mask[i, j] = True\n",
        "\n",
        "    return mask"
      ],
      "metadata": {
        "id": "tN_82CzxP5jQ"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def batch_hard_triplet_loss(labels, embeddings, margin, squared=False):\n",
        "    \"\"\"Build the triplet loss over a batch of embeddings.\n",
        "\n",
        "    For each anchor, we get the hardest positive and hardest negative to form a triplet.\n",
        "\n",
        "    Args:\n",
        "        labels: labels of the batch, of size (batch_size,)\n",
        "        embeddings: tensor of shape (batch_size, embed_dim)\n",
        "        margin: margin for triplet loss\n",
        "        squared: Boolean. If true, output is the pairwise squared euclidean distance matrix.\n",
        "                 If false, output is the pairwise euclidean distance matrix.\n",
        "\n",
        "    Returns:\n",
        "        triplet_loss: scalar tensor containing the triplet loss\n",
        "    \"\"\"\n",
        "    # Get the pairwise distance matrix\n",
        "    pairwise_dist = _pairwise_distances(embeddings, squared=squared)\n",
        "\n",
        "    # For each anchor, get the hardest positive\n",
        "    # First, we need to get a mask for every valid positive (they should have same label)\n",
        "    mask_anchor_positive = _get_anchor_positive_triplet_mask(labels)\n",
        "    mask_anchor_positive = mask_anchor_positive.float()\n",
        "\n",
        "    # We put to 0 any element where (a, p) is not valid (valid if a != p and label(a) == label(p))\n",
        "    anchor_positive_dist = torch.mul(mask_anchor_positive, pairwise_dist)\n",
        "\n",
        "    # shape (batch_size, 1)\n",
        "    hardest_positive_dist, _ = torch.max(anchor_positive_dist, dim=1, keepdim=True)\n",
        "\n",
        "    # For each anchor, get the hardest negative\n",
        "    # First, we need to get a mask for every valid negative (they should have different labels)\n",
        "    mask_anchor_negative = _get_anchor_negative_triplet_mask(labels)\n",
        "    mask_anchor_negative = mask_anchor_negative.float()\n",
        "\n",
        "    # We add the maximum value in each row to the invalid negatives (label(a) == label(n))\n",
        "    max_anchor_negative_dist, _ = torch.max(pairwise_dist, dim=1, keepdim=True)\n",
        "    anchor_negative_dist = pairwise_dist + max_anchor_negative_dist * (1.0 - mask_anchor_negative)\n",
        "\n",
        "    # shape (batch_size,)\n",
        "    hardest_negative_dist, _ = torch.min(anchor_negative_dist, dim=1, keepdim=True)\n",
        "\n",
        "    # Combine biggest d(a, p) and smallest d(a, n) into final triplet loss\n",
        "    triplet_loss = torch.max(hardest_positive_dist - hardest_negative_dist + margin, torch.tensor(0.0))\n",
        "\n",
        "    # Get final mean triplet loss\n",
        "    triplet_loss = torch.mean(triplet_loss)\n",
        "\n",
        "    return triplet_loss"
      ],
      "metadata": {
        "id": "B0NL6ZptJZ-g"
      },
      "execution_count": 15,
      "outputs": []
    }
  ]
}